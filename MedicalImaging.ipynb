{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MedicalImaging.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VdJhwDjBb1U_",
        "45zKW1NjS4aV",
        "EtnanhTdTS34"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nim7KW49HbU5",
        "colab_type": "text"
      },
      "source": [
        "![](https://storage.googleapis.com/kaggle-competitions/kaggle/10338/logos/header.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WtgHH-DbcRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Run this to download data and prepare our environment! { display-mode: \"form\" }\n",
        "def augment(data, augmenter):\n",
        "  if len(data.shape) == 3:\n",
        "    return augmenter.augment_image(data)\n",
        "  if len(data.shape) == 4:\n",
        "    return augmenter.augment_images(data)\n",
        "    \n",
        "def rotate(data, rotate):\n",
        "  fun = augmenters.Affine(rotate = rotate)\n",
        "  return augment(data, fun)\n",
        "\n",
        "def shear(data, shear):\n",
        "  fun = augmenters.Affine(shear = shear)\n",
        "  return augment(data, fun)\n",
        "\n",
        "def scale(data, scale):\n",
        "  fun = augmenters.Affine(scale = scale)\n",
        "  return augment(data, fun)\n",
        "  \n",
        "def flip_left_right(data):\n",
        "  fun = augmenters.Fliplr()\n",
        "  return augment(data, fun)\n",
        "\n",
        "def flip_up_down(data):\n",
        "  fun = augmenters.Flipud()\n",
        "  return augment(data, fun)\n",
        "\n",
        "def remove_color(data, channel):\n",
        "  new_data = data.copy()\n",
        "  if len(data.shape) == 3:\n",
        "    new_data[:,:,channel] = 0\n",
        "    return new_data\n",
        "  if len(data.shape) == 4:\n",
        "    new_data[:,:,:,channel] = 0\n",
        "    return new_data\n",
        "  \n",
        "class pkg:\n",
        "  #### DOWNLOADING AND LOADING DATA\n",
        "  def get_metadata(metadata_path, which_splits = ['train', 'test']):  \n",
        "    '''returns metadata dataframe which contains columns of:\n",
        "       * index: index of data into numpy data\n",
        "       * class: class of image\n",
        "       * split: which dataset split is this a part of? \n",
        "    '''\n",
        "    metadata = pd.read_csv(metadata_path)\n",
        "    keep_idx = metadata['split'].isin(which_splits)\n",
        "    return metadata[keep_idx]\n",
        "\n",
        "  def get_data_split(split_name, flatten, all_data, metadata, image_shape):\n",
        "    '''\n",
        "    returns images (data), labels from folder of format [image_folder]/[split_name]/[class_name]/\n",
        "    flattens if flatten option is True \n",
        "    '''\n",
        "    sub_df = metadata[metadata['split'].isin([split_name])]\n",
        "    index  = sub_df['index'].values\n",
        "    labels = sub_df['class'].values\n",
        "    data = all_data[index,:]\n",
        "    if flatten:\n",
        "      data = data.reshape([-1, np.product(image_shape)])\n",
        "    return data, labels\n",
        "\n",
        "  def get_train_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('train', flatten, all_data, metadata, image_shape)\n",
        "\n",
        "  def get_test_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('test', flatten, all_data, metadata, image_shape)\n",
        "\n",
        "  def get_field_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('field', flatten, all_data, metadata, image_shape)\n",
        "  \n",
        "class helpers:\n",
        "  #### PLOTTING\n",
        "  def plot_one_image(data, labels = [], index = None, image_shape = [64,64,3]):\n",
        "    '''\n",
        "    if data is a single image, display that image\n",
        "\n",
        "    if data is a 4d stack of images, display that image\n",
        "    '''\n",
        "    num_dims   = len(data.shape)\n",
        "    num_labels = len(labels)\n",
        "\n",
        "    # reshape data if necessary\n",
        "    if num_dims == 1:\n",
        "      data = data.reshape(target_shape)\n",
        "    if num_dims == 2:\n",
        "      data = data.reshape(np.vstack[-1, image_shape])\n",
        "    num_dims   = len(data.shape)\n",
        "\n",
        "    # check if single or multiple images\n",
        "    if num_dims == 3:\n",
        "      if num_labels > 1:\n",
        "        print('Multiple labels does not make sense for single image.')\n",
        "        return\n",
        "\n",
        "      label = labels      \n",
        "      if num_labels == 0:\n",
        "        label = ''\n",
        "      image = data\n",
        "\n",
        "    if num_dims == 4:\n",
        "      image = data[index, :]\n",
        "      label = labels[index]\n",
        "\n",
        "    # plot image of interest\n",
        "    print('Label: %s'%label)\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "  #### QUERYING AND COMBINING DATA\n",
        "  def get_misclassified_data(data, labels, predictions):\n",
        "    '''\n",
        "    Gets the data and labels that are misclassified in a classification task\n",
        "    Returns:\n",
        "    -missed_data\n",
        "    -missed_labels\n",
        "    -predicted_labels (corresponding to missed_labels)\n",
        "    -missed_index (indices of items in original dataset)\n",
        "    '''\n",
        "    missed_index     = np.where(np.abs(predictions.squeeze() - labels.squeeze()) > 0)[0]\n",
        "    missed_labels    = labels[missed_index]\n",
        "    missed_data      = data[missed_index,:]\n",
        "    predicted_labels = predictions[missed_index]\n",
        "    return missed_data, missed_labels, predicted_labels, missed_index\n",
        "\n",
        "  def combine_data(data_list, labels_list):\n",
        "    return np.concatenate(data_list, axis = 0), np.concatenate(labels_list, axis = 0)\n",
        "\n",
        "  def model_to_string(model):\n",
        "    import re\n",
        "    stringlist = []\n",
        "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
        "    sms = \"\\n\".join(stringlist)\n",
        "    sms = re.sub('_\\d\\d\\d','', sms)\n",
        "    sms = re.sub('_\\d\\d','', sms)\n",
        "    sms = re.sub('_\\d','', sms)  \n",
        "    return sms\n",
        "\n",
        "  def plot_acc(history, ax = None, xlabel = 'Epoch #'):\n",
        "    # i'm sorry for this function's code. i am so sorry. \n",
        "    history = history.history\n",
        "    history.update({'epoch':list(range(len(history['val_acc'])))})\n",
        "    history = pd.DataFrame.from_dict(history)\n",
        "\n",
        "    best_epoch = history.sort_values(by = 'val_acc', ascending = False).iloc[0]['epoch']\n",
        "\n",
        "    if not ax:\n",
        "      f, ax = plt.subplots(1,1)\n",
        "    sns.lineplot(x = 'epoch', y = 'val_acc', data = history, label = 'Validation', ax = ax)\n",
        "    sns.lineplot(x = 'epoch', y = 'acc', data = history, label = 'Training', ax = ax)\n",
        "    ax.axhline(0.5, linestyle = '--',color='red', label = 'Chance')\n",
        "    ax.axvline(x = best_epoch, linestyle = '--', color = 'green', label = 'Best Epoch')  \n",
        "    ax.legend(loc = 1)    \n",
        "    ax.set_ylim([0.4, 1])\n",
        "\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel('Accuracy (Fraction)')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "class models:\n",
        "  def DenseClassifier(hidden_layer_sizes, nn_params):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape = nn_params['input_shape']))\n",
        "    for ilayer in hidden_layer_sizes:\n",
        "      model.add(Dense(ilayer, activation = 'relu'))\n",
        "    model.add(Dense(units = nn_params['output_neurons'], activation = nn_params['output_activation']))\n",
        "    model.compile(loss=nn_params['loss'],\n",
        "                  optimizer=optimizers.SGD(lr=1e-4, momentum=0.95),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  def CNNClassifier(num_hidden_layers, nn_params):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=nn_params['input_shape'], padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    for i in range(num_hidden_layers-1):\n",
        "        model.add(Conv2D(32, (3, 3), padding = 'same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten()) \n",
        "\n",
        "    model.add(Dense(units = 128, activation = 'relu'))\n",
        "\n",
        "    model.add(Dense(units = 64, activation = 'relu'))\n",
        "\n",
        "\n",
        "    model.add(Dense(units = nn_params['output_neurons'], activation = nn_params['output_activation']))\n",
        "\n",
        "    # initiate RMSprop optimizer\n",
        "    opt = keras.optimizers.RMSprop(lr=1e-4, decay=1e-6)\n",
        "\n",
        "    # Let's train the model using RMSprop\n",
        "    model.compile(loss=nn_params['loss'],\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])    \n",
        "    return model\n",
        "\n",
        "  def TransferClassifier(name, nn_params, trainable = False):\n",
        "    expert_dict = {'VGG16': VGG16, \n",
        "                   'VGG19': VGG19,\n",
        "                   'ResNet50':ResNet50,\n",
        "                   'DenseNet121':DenseNet121}\n",
        "\n",
        "    expert_conv = expert_dict[name](weights = 'imagenet', \n",
        "                                              include_top = False, \n",
        "                                              input_shape = nn_params['input_shape'])\n",
        "    for layer in expert_conv.layers:\n",
        "      layer.trainable = trainable\n",
        "      \n",
        "    expert_model = Sequential()\n",
        "    expert_model.add(expert_conv)\n",
        "    expert_model.add(GlobalAveragePooling2D())\n",
        "\n",
        "    expert_model.add(Dense(128, activation = 'relu'))\n",
        "    expert_model.add(Dropout(0.3))\n",
        "\n",
        "    expert_model.add(Dense(64, activation = 'relu'))\n",
        "\n",
        "    expert_model.add(Dense(nn_params['output_neurons'], activation = nn_params['output_activation']))\n",
        "\n",
        "    expert_model.compile(loss = nn_params['loss'], \n",
        "                  optimizer = optimizers.SGD(lr=1e-4, momentum=0.95), \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return expert_model\n",
        "\n",
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn import model_selection\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape, Dense, Conv2D, GlobalAveragePooling2D\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import keras.optimizers as optimizers\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.applications import VGG16, VGG19, ResNet50, DenseNet121\n",
        "\n",
        "from imgaug import augmenters \n",
        "\n",
        "### defining project variables\n",
        "# file variables\n",
        "image_data_url       = 'https://drive.google.com/uc?id=1DNEiLAWguswhiLXGyVKsgHIRm1xZggt_'\n",
        "metadata_url         = 'https://drive.google.com/uc?id=1MW3_FU6qc0qT_uG4bzxhtEHy4Jd6dCWb'\n",
        "image_data_path      = './image_data.npy'\n",
        "metadata_path        = './metadata.csv'\n",
        "image_shape          = (64, 64, 3)\n",
        "\n",
        "# neural net parameters\n",
        "nn_params = {}\n",
        "nn_params['input_shape']       = image_shape\n",
        "nn_params['output_neurons']    = 1\n",
        "nn_params['loss']              = 'binary_crossentropy'\n",
        "nn_params['output_activation'] = 'sigmoid'\n",
        "\n",
        "###\n",
        "gdown.download(image_data_url, './image_data.npy', True)\n",
        "gdown.download(metadata_url, './metadata.csv', True)\n",
        "\n",
        "### pre-loading all data of interest\n",
        "_all_data = np.load('image_data.npy')\n",
        "_metadata = pkg.get_metadata(metadata_path, ['train','test','field'])\n",
        "\n",
        "### preparing definitions\n",
        "# downloading and loading data\n",
        "get_data_split = pkg.get_data_split\n",
        "get_metadata    = lambda :                 pkg.get_metadata(metadata_path, ['train','test'])\n",
        "get_train_data  = lambda flatten = False : pkg.get_train_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "get_test_data   = lambda flatten = False : pkg.get_test_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "get_field_data  = lambda flatten = False : pkg.get_field_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "\n",
        "# plotting\n",
        "plot_one_image = lambda data, labels = [], index = None: helpers.plot_one_image(data = data, labels = labels, index = index, image_shape = image_shape);\n",
        "plot_acc       = lambda history: helpers.plot_acc(history)\n",
        "\n",
        "# querying and combining data\n",
        "model_to_string        = lambda model: helpers.model_to_string(model)\n",
        "get_misclassified_data = helpers.get_misclassified_data;\n",
        "combine_data           = helpers.combine_data;\n",
        "\n",
        "# models with input parameters\n",
        "DenseClassifier     = lambda hidden_layer_sizes: models.DenseClassifier(hidden_layer_sizes = hidden_layer_sizes, nn_params = nn_params);\n",
        "CNNClassifier       = lambda num_hidden_layers: models.CNNClassifier(num_hidden_layers, nn_params = nn_params);\n",
        "TransferClassifier  = lambda name: models.TransferClassifier(name = name, nn_params = nn_params);\n",
        "\n",
        "monitor = ModelCheckpoint('./model.h5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuUfPXfZNf7w",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 1: Putting our model into practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK4kEOlrNkLh",
        "colab_type": "text"
      },
      "source": [
        "## Activity 1a: Testing on Field Data\n",
        "#### While your models may have done well on your original training and validation data, deploying the model on \"field\" data can present different challenges. Field data is data that is different from the one where you built your model. For e.g. images obtained from a different x-ray machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXbX8EwYNn5i",
        "colab_type": "text"
      },
      "source": [
        "### Exercise (Coding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OcX3HejNrjJ",
        "colab_type": "text"
      },
      "source": [
        "Yesterday we worked with CNNs, which we saw perform much better than multi-layer perceptrons on imaging data. \n",
        "\n",
        "Re-train a CNNClassifier with 5 hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLos8L1KnCe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "cnn_old = CNNClassifier(5)\n",
        "\n",
        "train_data, train_labels = get_train_data()\n",
        "test_data, test_labels   = get_test_data()\n",
        "\n",
        "cnn_old.fit(train_data, train_labels, epochs = 20, validation_data = (test_data, test_labels), shuffle = True, callbacks = [monitor])\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aoowqx5Np9n",
        "colab_type": "text"
      },
      "source": [
        "### Exercise (Coding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpWnKw_IPIr1",
        "colab_type": "text"
      },
      "source": [
        "Our radiologist friends have provided some data from the field. We can access this with\n",
        "```\n",
        "field_data, field_labels = get_field_data()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fdGge38Q3iH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "field_data, field_labels = get_field_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JncI7hnrPQS5",
        "colab_type": "text"
      },
      "source": [
        "Use `cnn.predict_classes(field_data)` to get your predictions. \n",
        "\n",
        "**Calculate: How well does our `cnn` model do on the field data?**\n",
        "\n",
        "**IMPORTANT: Re-train your model on the field data 5 times and print the average accuracy over the 5 runs.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7S7rncnQFSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "train_data, train_labels = get_train_data()\n",
        "test_data, test_labels   = get_test_data()\n",
        "field_data, field_labels = get_field_data()\n",
        "  \n",
        "average_accuracy = 0.0\n",
        "for i in range(5):\n",
        "  cnn_old = CNNClassifier(5)\n",
        "  cnn_old.fit(train_data, train_labels, epochs = 20, validation_data = (test_data, test_labels), shuffle = True, callbacks = [monitor])\n",
        "  \n",
        "  predictions = cnn_old.predict_classes(field_data)\n",
        "  accuracy = accuracy_score(field_labels, predictions)\n",
        "  print('Accuracy on this run: %0.2f' % accuracy)\n",
        "  \n",
        "  average_accuracy += accuracy / 5.0\n",
        "print('Average accuracy: ', average_accuracy)\n",
        "\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf7xfFv9PwrF",
        "colab_type": "text"
      },
      "source": [
        "## Activity 1b: Understanding our model's performance on field data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdJhwDjBb1U_",
        "colab_type": "text"
      },
      "source": [
        "### Exercise (Discussion)\n",
        "Discuss this with your instructor:\n",
        "* How did your model do? Did it perform quite as well?\n",
        "* Why do you think it did this way? \n",
        "* **Come up with a few hypotheses for what's different between our test data and our field data!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pILpz1SScRRl",
        "colab_type": "text"
      },
      "source": [
        "## Activity 1c: Error analysis\n",
        "\n",
        "### Understanding where the model did not perform as well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjo7cW5kP14m",
        "colab_type": "text"
      },
      "source": [
        "### Exercise (Coding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOmLO3M4P3XO",
        "colab_type": "text"
      },
      "source": [
        "You can use the function `get_misclassified_data` to find the data where our labels are misclassifed. To use this function...\n",
        "\n",
        "\n",
        "```\n",
        "[missed_data, missed_labels, predicted_labels, missed_indices] = get_misclassified_data(data, labels, predictions)\n",
        "```\n",
        "\n",
        "where\n",
        "\n",
        "* `missed_data`: the images that our model misclassified\n",
        "* `missed_labels`: the labels corresponding to those images\n",
        "* `predicted_labels`: what the model thought the labels were\n",
        "* `missed_indices`: where in our original dataset these images were\n",
        "\n",
        "\n",
        "Step 1: Use `get_misclassified_data` to get the misclassified images.\n",
        "\n",
        "Step 2: Use `plot_one_image(data, labels, index)` to visualize them. Hint: a for loop could help you visualize many such images!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K8hWwOScv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "### Get the misclassified data \n",
        "[missed_data, missed_labels, predicted_labels, missed_indices] = get_misclassified_data(field_data, field_labels, predictions)\n",
        "\n",
        "print('How many bad images?')\n",
        "print(missed_data.shape[0])\n",
        "\n",
        "print('Which labels are bad?')\n",
        "print(missed_indices)\n",
        "\n",
        "for i in range(len(missed_data)):\n",
        "  print(predicted_labels[i])\n",
        "  plot_one_image(missed_data, missed_labels, i)\n",
        "  \n",
        "\n",
        "### Viusialize the images \n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45zKW1NjS4aV",
        "colab_type": "text"
      },
      "source": [
        "## Instructor-Led Discussion: What is happening in our field data? \n",
        "\n",
        "Discuss in your group and with your instructor!\n",
        "\n",
        "Why is our model performance suffering?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NhR3Mb0Su9U",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 2: Tools to improve your models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhYKVx5nTL2A",
        "colab_type": "text"
      },
      "source": [
        "## Activity 2a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtnanhTdTS34",
        "colab_type": "text"
      },
      "source": [
        "### Instructor-Led Discussion: Data Augmentation\n",
        "#### To improve the variability in our training data, we sometimes make smart alterations of our data and add to the training dataset, so that we can improve our model's performance on such 'field' datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Riq7peVmkUhA",
        "colab_type": "text"
      },
      "source": [
        "### Exercise (Coding) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ-CtwWseVu_",
        "colab_type": "text"
      },
      "source": [
        "So, how do we augment our images in Python? \n",
        "\n",
        "\n",
        "We provide custom functions for augmenting a single image. \n",
        "* `rotate(image, rotate = [0, 30])`\n",
        "* `scale(image, scale = [1, 1.5])`\n",
        "* `shear(image, shear = [0 20])`\n",
        "* `flip_left_right(image, prob = 0.5)`\n",
        "* `flip_up_down(image, prob = 0.5)`\n",
        "* `remove_color(data, channel = 0)`\n",
        "\n",
        "\n",
        "Below, we show you how to use rotate on a single image! Try some of these options out yourself and augment your dataset with the augmenters. \n",
        "\n",
        "**Share some of your cool augmentation strategies with the class! You can definitely use multiple augmentation techniques for each image!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOEkYlf5gquP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "## This loads in an image from the training data.\n",
        "## The data is of shape (num_images, width, height, color_channels).\n",
        "## In order to get a different image, change the variable below.\n",
        "IMAGE_INDEX = 0\n",
        "image = train_data[IMAGE_INDEX,:,:,:]\n",
        "plot_one_image(shear(image, shear = 10))\n",
        "plot_one_image(flip_left_right(image))\n",
        "plot_one_image(flip_up_down(image))\n",
        "plot_one_image(rotate(image, rotate = 90))\n",
        "plot_one_image(remove_color(image, channel = 1))\n",
        "\n",
        "## Now run some augmentations on the image and use plot_one_image to visualize them.\n",
        "\n",
        "\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihblU152Wapd",
        "colab_type": "text"
      },
      "source": [
        "## Activity 2b. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfVDv1P3abu_",
        "colab_type": "text"
      },
      "source": [
        "### Exercise (Coding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9hUgRMfWefl",
        "colab_type": "text"
      },
      "source": [
        "We want to add our own custom augmentations to our training data. Below, you should:\n",
        "* Define a set of augmentations to do on your training data\n",
        "* Augment a subset (or all) of the training data with these augmentations\n",
        "* Combine the augmented data with your train data to make a new data set\n",
        "\n",
        "\n",
        "To combine your original train data with your augmented data, you can use the `combine_data` function like:\n",
        "```\n",
        "all_data, all_labels = combine_data([data1, data2], [labels1, labels2])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C7yqCoAW5qs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "### Perform an augmentation.\n",
        "train_data_rotated = rotate(train_data, rotate=90)\n",
        "train_data_rotated_180 = rotate(train_data, rotate=180)\n",
        "train_data_rotated_270 = rotate(train_data, rotate=270)\n",
        "\n",
        "\n",
        "### TODO: Perform one or more additional augmentations.\n",
        "\n",
        "# make red images\n",
        "red_train  = remove_color(remove_color(train_data[:1000], channel = 1), channel = 2)\n",
        "red_labels   = train_labels[:1000]\n",
        "\n",
        "# make rotated images\n",
        "rotate_train    = rotate(train_data[1000:2000], rotate = 90)\n",
        "rotate_labels   = train_labels[1000:2000]\n",
        "\n",
        "\n",
        "## Combine data. TODO: Add more data and corresponding labels here!\n",
        "all_data, all_labels = combine_data([train_data, train_data_rotated, train_data_rotated_180], \\\n",
        "                                    [train_labels, train_labels, train_labels])\n",
        "\n",
        "\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4y0BFbJYf1k",
        "colab_type": "text"
      },
      "source": [
        "Once you've created your augmented data...\n",
        "1. Train your CNN on `all_data` \n",
        "2. Choose the best epoch based on the `test_data`! \n",
        "3. Load your model up and score it on `field_data`. \n",
        "\n",
        "**Your challenge is to find a set of augmentations that improves your model's performance on the `field_data`! Share your augmentations and performances with the class! Try as many or as few augmentations as you want.**\n",
        "\n",
        "**Remember to record an average of 5 newly initialized CNNs. This is important because CNN weights will be initialized differently in each run!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v453qnBB-Y9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "train_data, train_labels = get_train_data()\n",
        "test_data, test_labels   = get_test_data()\n",
        "field_data, field_labels   = get_field_data()\n",
        "average_accuracy = 0.0\n",
        "\n",
        "for i in range(5):\n",
        "  cnn = CNNClassifier(5) \n",
        "cnn.fit(all_data, all_labels, epochs = 20, validation_data = (test_data, test_labels), shuffle = True, callbacks = [monitor])\n",
        "  predictions = cnn.predict_classes(field_data)\n",
        "  accuracy = accuracy_score(field_labels, predictions)\n",
        "  print('Accuracy:%0.2f'%accuracy)\n",
        "  average_accuracy += accuracy\n",
        "\n",
        "average_accuracy /= 5.0\n",
        "print('Average accuracy: ', average_accuracy)\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz1pSff8xXWk",
        "colab_type": "text"
      },
      "source": [
        "## Instructor-Led Discussion: Why does data augmentation improve average performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LkaHktp2L01",
        "colab_type": "text"
      },
      "source": [
        "![](https://storage.googleapis.com/kaggle-competitions/kaggle/10338/logos/header.png)"
      ]
    }
  ]
}